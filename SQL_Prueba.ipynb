{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63499525-d2d6-4c3b-87f4-81f9345668d0",
   "metadata": {},
   "source": [
    "# SQL para Data Science"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80449913-9d2d-4e88-9d7a-5e9a3ee08af3",
   "metadata": {},
   "source": [
    "## Prueba final de módulo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14a3f8e-d54f-43b9-ad02-562d937fea94",
   "metadata": {},
   "source": [
    "### Francisca Pinto | 14 de diciembre de 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e247b5df-0cf4-4770-951d-1503e2ee615b",
   "metadata": {},
   "source": [
    "## Parte I\n",
    "\n",
    "**Parte 1: Registro de los archivos en la base de datos. (3 Puntos)**\n",
    "1. Generar una nueva base de datos con la siguiente nomenclatura: apellido_nombre.\n",
    "2. Importar en tablas los archivos train_cupid.csv y test_cupid.csv a un motor Postgres, implementando sólo la librería psycopg2. Las tablas deben contener los nombres de las columnas y el total de los registros presente en cada archivo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d0cdb8-a22b-49af-924f-2093ce313c11",
   "metadata": {},
   "source": [
    "Para partir, se importan las librerías correspondientes, junto con la instalación del módulo <code>autotime</code> que informa sobre el tiempo de ejecución de cada celda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c06eab5-14b0-4cf1-8f93-a51348128014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipython-autotime in c:\\users\\fnpin\\.conda\\envs\\adl\\lib\\site-packages (0.3.1)\n",
      "Requirement already satisfied: ipython in c:\\users\\fnpin\\.conda\\envs\\adl\\lib\\site-packages (from ipython-autotime) (7.22.0)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\fnpin\\.conda\\envs\\adl\\lib\\site-packages (from ipython->ipython-autotime) (0.7.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\fnpin\\.conda\\envs\\adl\\lib\\site-packages (from ipython->ipython-autotime) (0.4.4)\n",
      "Requirement already satisfied: traitlets>=4.2 in c:\\users\\fnpin\\.conda\\envs\\adl\\lib\\site-packages (from ipython->ipython-autotime) (5.0.5)\n",
      "Requirement already satisfied: backcall in c:\\users\\fnpin\\.conda\\envs\\adl\\lib\\site-packages (from ipython->ipython-autotime) (0.2.0)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\fnpin\\.conda\\envs\\adl\\lib\\site-packages (from ipython->ipython-autotime) (0.17.2)\n",
      "Requirement already satisfied: pygments in c:\\users\\fnpin\\.conda\\envs\\adl\\lib\\site-packages (from ipython->ipython-autotime) (2.9.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in c:\\users\\fnpin\\.conda\\envs\\adl\\lib\\site-packages (from ipython->ipython-autotime) (3.0.17)\n",
      "Requirement already satisfied: setuptools>=18.5 in c:\\users\\fnpin\\.conda\\envs\\adl\\lib\\site-packages (from ipython->ipython-autotime) (52.0.0.post20210125)\n",
      "Requirement already satisfied: decorator in c:\\users\\fnpin\\.conda\\envs\\adl\\lib\\site-packages (from ipython->ipython-autotime) (5.0.9)\n",
      "Requirement already satisfied: parso<0.8.0,>=0.7.0 in c:\\users\\fnpin\\.conda\\envs\\adl\\lib\\site-packages (from jedi>=0.16->ipython->ipython-autotime) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\fnpin\\.conda\\envs\\adl\\lib\\site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->ipython-autotime) (0.2.5)\n",
      "Requirement already satisfied: ipython-genutils in c:\\users\\fnpin\\.conda\\envs\\adl\\lib\\site-packages (from traitlets>=4.2->ipython->ipython-autotime) (0.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install ipython-autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64505724-b719-4713-9892-897c914595ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 6.51 s (started: 2021-12-14 00:05:35 -03:00)\n"
     ]
    }
   ],
   "source": [
    "#tiempo de ejecución\n",
    "%load_ext autotime\n",
    "\n",
    "#extensiones para trabajo con postgreSQL\n",
    "import psycopg2\n",
    "from sqlalchemy import *\n",
    "\n",
    "#dataframe y arrays\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#estadística\n",
    "import scipy.stats as stats\n",
    "\n",
    "#separación entrenamiento validación\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#regresión logística\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#NaiveBayes\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "#SVC\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "#árboles de clasificación\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "#Árbol de decisión de clasificación\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#AdaBoost Clasiffier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "#lectura de archivos csv\n",
    "import csv\n",
    "\n",
    "#serialización de modelos\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ffac33-0704-47d6-ab97-4b05853f5ee2",
   "metadata": {},
   "source": [
    "Ahora se realizará la conexión a la base de datos <code>apellido_nombre</code> ya solicitada. Esta fue creada en la <code>SQL Shell</code> como se muestra en el archivo de texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77104f7d-7f88-4236-877e-2a05f889cbb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 62 ms (started: 2021-12-14 00:05:41 -03:00)\n"
     ]
    }
   ],
   "source": [
    "#conexión a base de datos\n",
    "\n",
    "conn = psycopg2.connect(\n",
    "                        dbname = \"pinto_francisca\",\n",
    "                        port = \"5432\",\n",
    "                        user = \"postgres\",\n",
    "                        password = \"1163143094161049\"\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1430d73-7902-488b-8792-2d052dc66f3e",
   "metadata": {},
   "source": [
    "Para crear cada tabla se utilizarán los nombres de columnas de cada archivo <code>.csv</code> además de las filas que contienen la información necesaria.\n",
    "\n",
    "Para lo primero, se iterará sobre el archivo <code>test_cupid.csv</code> y se creará una nueva variable <code>header</code> con los encabezados con el método <code>next()</code>, que nos permiten extraer la información en la primera iteración."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8475b3a8-21ad-4135-a3b3-cd6ff1086eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['age', 'height', 'virgo', 'taurus', 'scorpio', 'pisces', 'libra', 'leo', 'gemini', 'aries', 'aquarius', 'cancer', 'sagittarius', 'asian', 'hispanic / latin', 'black', 'indian', 'pacific islander', 'native american', 'middle eastern', 'colorado', 'new york', 'oregon', 'arizona', 'hawaii', 'montana', 'wisconsin', 'virginia', 'spain', 'nevada', 'illinois', 'vietnam', 'ireland', 'louisiana', 'michigan', 'texas', 'united kingdom', 'massachusetts', 'north carolina', 'idaho', 'mississippi', 'new jersey', 'florida', 'minnesota', 'georgia', 'utah', 'washington', 'west virginia', 'connecticut', 'tennessee', 'rhode island', 'district of columbia', 'canada', 'missouri', 'germany', 'pennsylvania', 'netherlands', 'switzerland', 'mexico', 'ohio', 'agnosticism', 'atheism', 'catholicism', 'buddhism', 'judaism', 'hinduism', 'islam', 'pro_dogs', 'pro_cats', 'spanish', 'chinese', 'french', 'german', 'single', 'seeing_someone', 'available', 'employed', 'income_between_25_50', 'income_between_50_75', 'income_over_75', 'drugs_often', 'drugs_sometimes', 'drinks_not at all', 'drinks_often', 'drinks_rarely', 'drinks_socially', 'drinks_very often', 'orientation_gay', 'orientation_straight', 'sex_m', 'smokes_sometimes', 'smokes_trying to quit', 'smokes_when drinking', 'smokes_yes', 'body_type_overweight', 'body_type_regular', 'education_high_school', 'education_undergrad_university']\n",
      "time: 0 ns (started: 2021-12-14 00:05:41 -03:00)\n"
     ]
    }
   ],
   "source": [
    "# ingesta de columnas de archivos\n",
    "\n",
    "with open(\"./test_cupid.csv\",\n",
    "          \"r\") as file_header:\n",
    "    reader_cols = csv.reader(file_header)\n",
    "    header = next(reader_cols)\n",
    "    \n",
    "file_header.close()\n",
    "print(header)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a43a44-2a69-44d9-ac2c-a765610f7857",
   "metadata": {},
   "source": [
    "Se continúa con la eliminación del slash y espacios en variables por errores en la lectura posterior con <code>.replace()</code>. Posteriormente, se sobreescribirá la variable que contiene esta información para luego utilizarla con <code>.execute(CREATE TABLE)</code>, ya que de otra forma se generará un error (no es posible incorporar información con espacios y <code>/</code>)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9155f48f-25d9-4cf1-8906-f294e6661085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns (started: 2021-12-14 00:05:41 -03:00)\n"
     ]
    }
   ],
   "source": [
    "header = list(map(lambda x: x.replace(\" \", \"_\").replace(\"/\", \"\"), header))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6bb4a2b6-656b-4e40-b849-7d7860680b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['age', 'height', 'virgo', 'taurus', 'scorpio', 'pisces', 'libra', 'leo', 'gemini', 'aries', 'aquarius', 'cancer', 'sagittarius', 'asian', 'hispanic__latin', 'black', 'indian', 'pacific_islander', 'native_american', 'middle_eastern', 'colorado', 'new_york', 'oregon', 'arizona', 'hawaii', 'montana', 'wisconsin', 'virginia', 'spain', 'nevada', 'illinois', 'vietnam', 'ireland', 'louisiana', 'michigan', 'texas', 'united_kingdom', 'massachusetts', 'north_carolina', 'idaho', 'mississippi', 'new_jersey', 'florida', 'minnesota', 'georgia', 'utah', 'washington', 'west_virginia', 'connecticut', 'tennessee', 'rhode_island', 'district_of_columbia', 'canada', 'missouri', 'germany', 'pennsylvania', 'netherlands', 'switzerland', 'mexico', 'ohio', 'agnosticism', 'atheism', 'catholicism', 'buddhism', 'judaism', 'hinduism', 'islam', 'pro_dogs', 'pro_cats', 'spanish', 'chinese', 'french', 'german', 'single', 'seeing_someone', 'available', 'employed', 'income_between_25_50', 'income_between_50_75', 'income_over_75', 'drugs_often', 'drugs_sometimes', 'drinks_not_at_all', 'drinks_often', 'drinks_rarely', 'drinks_socially', 'drinks_very_often', 'orientation_gay', 'orientation_straight', 'sex_m', 'smokes_sometimes', 'smokes_trying_to_quit', 'smokes_when_drinking', 'smokes_yes', 'body_type_overweight', 'body_type_regular', 'education_high_school', 'education_undergrad_university']\n",
      "time: 15 ms (started: 2021-12-14 18:10:58 -03:00)\n"
     ]
    }
   ],
   "source": [
    "print(header) #se imprime resultado para corroborar efectos de función lambda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04eb3ce-d86d-49b3-a7f8-f2d10d71cc7d",
   "metadata": {},
   "source": [
    "Ahora se genera la variable que es un <code>string</code> con la solicitud que se realizará a la base de datos en <code>postreSQL</code> para la creación de nueva tabla con las observaciones de <code>test_cupid</code>, y se realiza un procedimiento homólogo para <code>train_cupid</code> solo modificando el nombre de tabla (los atributos son los mismos). Se utilizará <code>.format()</code> para incorporar los nombres de cada variable en función de <code>header</code> para no tener que escribir cada nombre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c03157df-b235-4b7c-a264-769479e0e7ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns (started: 2021-12-14 00:05:41 -03:00)\n"
     ]
    }
   ],
   "source": [
    "cols_query_test = f\"\"\"CREATE TABLE test_cupid(\n",
    "             {header[0]} integer,\n",
    "             {header[1]} decimal,\n",
    "             {header[2]} integer,\n",
    "             {header[3]} integer,\n",
    "             {header[4]} integer,\n",
    "             {header[5]} integer,\n",
    "             {header[6]} integer,\n",
    "             {header[7]} integer,\n",
    "             {header[8]} integer,\n",
    "             {header[9]} integer,\n",
    "             {header[10]} integer,\n",
    "             {header[11]} integer,\n",
    "             {header[12]} integer,\n",
    "             {header[13]} integer,\n",
    "             {header[14]} integer,\n",
    "             {header[15]} integer,\n",
    "             {header[16]} integer,\n",
    "             {header[17]} integer,\n",
    "             {header[18]} integer,\n",
    "             {header[19]} integer,\n",
    "             {header[20]} integer,\n",
    "             {header[21]} integer,\n",
    "             {header[22]} integer,\n",
    "             {header[23]} integer,\n",
    "             {header[24]} integer,\n",
    "             {header[25]} integer,\n",
    "             {header[26]} integer,\n",
    "             {header[27]} integer,\n",
    "             {header[28]} integer,\n",
    "             {header[29]} integer,\n",
    "             {header[30]} integer,\n",
    "             {header[31]} integer,\n",
    "             {header[32]} integer,\n",
    "             {header[33]} integer,\n",
    "             {header[34]} integer,\n",
    "             {header[35]} integer,\n",
    "             {header[36]} integer,\n",
    "             {header[37]} integer,\n",
    "             {header[38]} integer,\n",
    "             {header[39]} integer,\n",
    "             {header[40]} integer,\n",
    "             {header[41]} integer,\n",
    "             {header[42]} integer,\n",
    "             {header[43]} integer,\n",
    "             {header[44]} integer,\n",
    "             {header[45]} integer,\n",
    "             {header[46]} integer,\n",
    "             {header[47]} integer,\n",
    "             {header[48]} integer,\n",
    "             {header[49]} integer,\n",
    "             {header[50]} integer,\n",
    "             {header[51]} integer,\n",
    "             {header[52]} integer,\n",
    "             {header[53]} integer,\n",
    "             {header[54]} integer,\n",
    "             {header[55]} integer,\n",
    "             {header[56]} integer,\n",
    "             {header[57]} integer,\n",
    "             {header[58]} integer,\n",
    "             {header[59]} integer,\n",
    "             {header[60]} integer,\n",
    "             {header[61]} integer,\n",
    "             {header[62]} integer,\n",
    "             {header[63]} integer,\n",
    "             {header[64]} integer,\n",
    "             {header[65]} integer,\n",
    "             {header[66]} integer,\n",
    "             {header[67]} decimal,\n",
    "             {header[68]} decimal,\n",
    "             {header[69]} integer,\n",
    "             {header[70]} integer,\n",
    "             {header[71]} integer,\n",
    "             {header[72]} integer,\n",
    "             {header[73]} integer,\n",
    "             {header[74]} integer,\n",
    "             {header[75]} integer,\n",
    "             {header[76]} integer,\n",
    "             {header[77]} integer,\n",
    "             {header[78]} integer,\n",
    "             {header[79]} integer,\n",
    "             {header[80]} integer,\n",
    "             {header[81]} integer,\n",
    "             {header[82]} integer,\n",
    "             {header[83]} integer,\n",
    "             {header[84]} integer,\n",
    "             {header[85]} integer,\n",
    "             {header[86]} integer,\n",
    "             {header[87]} integer,\n",
    "             {header[88]} integer,\n",
    "             {header[89]} integer,\n",
    "             {header[90]} integer,\n",
    "             {header[91]} integer,\n",
    "             {header[92]} integer,\n",
    "             {header[93]} integer,\n",
    "             {header[94]} integer,\n",
    "             {header[95]} integer,\n",
    "             {header[96]} integer,\n",
    "             {header[97]} integer\n",
    "             );\"\"\"\n",
    "\n",
    "cols_query_train = cols_query_test.replace(\"test_cupid\", \"train_cupid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bc7cd4-cab8-48ef-8ef6-a5805719cf79",
   "metadata": {},
   "source": [
    "Se abren ejecutan, ejecutan y persisten los cursores correspondientes, utlizando los <code>strings</code> recién creados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0aa5d631-44f2-45d0-9c40-d659ffd1274f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 16 ms (started: 2021-12-14 00:05:42 -03:00)\n"
     ]
    }
   ],
   "source": [
    "cur_test = conn.cursor()\n",
    "cur_train = conn.cursor()\n",
    "\n",
    "cur_test.execute(cols_query_test)\n",
    "\n",
    "cur_train.execute(cols_query_train)\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205bb726-c2c6-4aee-ab08-8e1860deaad9",
   "metadata": {},
   "source": [
    "Las tablas ya tienen sus encabezados, por lo que ahora pueden importarse los datos desde los archivos <code>.csv</code>.\n",
    "\n",
    "Nuevamente se creará una variable para la solicitud de modificación de tablas, con un <code>string</code> concatenado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1ac4b2d-1d8c-453a-9f84-eb0b7b3a9e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns (started: 2021-12-14 00:05:42 -03:00)\n"
     ]
    }
   ],
   "source": [
    "format_query = \"%s, \" * 97 + \"%s\"\n",
    "rows_query_test = f\"\"\"INSERT INTO test_cupid VALUES ({format_query})\"\"\"\n",
    "rows_query_train = f\"\"\"INSERT INTO train_cupid VALUES ({format_query})\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3030524-74e5-4638-9cad-581db74a5d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 17.8 s (started: 2021-12-14 00:05:42 -03:00)\n"
     ]
    }
   ],
   "source": [
    "# ingesta de columnas de archivos\n",
    "\n",
    "with open(\"./test_cupid.csv\",\n",
    "          \"r\") as file_test:\n",
    "    reader_test = csv.reader(file_test)\n",
    "    next(reader_test) #ahora se descarta esta información porque ya fue utilizada\n",
    "    \n",
    "    for row_test in reader_test:\n",
    "        cur_test.execute(rows_query_test, row_test)\n",
    "\n",
    "file_test.close()\n",
    "conn.commit()\n",
    "\n",
    "with open(\"./train_cupid.csv\",\n",
    "          \"r\") as file_train:\n",
    "    reader_train = csv.reader(file_train)\n",
    "    next(reader_train) #ahora se descarta esta información porque ya fue utilizada\n",
    "    \n",
    "    for row_train in reader_train:\n",
    "        cur_test.execute(rows_query_train, row_train)\n",
    "\n",
    "file_train.close()\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4367f3-a7e8-4e87-aa97-9dc9800cf6fb",
   "metadata": {},
   "source": [
    "El proceso de ingesta de <code>test_cupid</code> y <code>train_cupid</code> ha sido realizado correctamente, ahora se puede crear los dataframes correspondientes consultando a las tablas en la base de datos correspondiente. Pueden observarse los resultados de importación en el archivo de texto adjunto.\n",
    "\n",
    "Luego, se incorporan los nombres de columna a cada dataframe con la lista ya creada anteriormente, <code>header</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "337e0688-4a69-492a-9825-2996f8988a48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.59 s (started: 2021-12-14 00:05:59 -03:00)\n"
     ]
    }
   ],
   "source": [
    "cur_train.execute(\"SELECT * FROM train_cupid;\")\n",
    "cur_test.execute(\"SELECT * FROM test_cupid;\")\n",
    "\n",
    "cols_train = cur_train.fetchall()\n",
    "cols_test = cur_test.fetchall()\n",
    "\n",
    "train_cupid = pd.DataFrame(list(cols_train))\n",
    "test_cupid = pd.DataFrame(list(cols_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52ef711d-4045-4b38-aac4-c3ed3e711f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns (started: 2021-12-14 00:06:02 -03:00)\n"
     ]
    }
   ],
   "source": [
    "train_cupid.columns = header\n",
    "test_cupid.columns = header"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d661d67d-7a3c-4e26-8712-a38ecfc07fe1",
   "metadata": {},
   "source": [
    "**Parte 2: Entrenamiento de modelos (3.5 Puntos)**\n",
    "1. Ingestar la tabla de training mediante <code>psycopg2</code> para el posterior entrenamiento del modelo.\n",
    "2. Entrenar los siguientes modelos (sin necesidad de ajustar por hiper parámetros):\n",
    "* GradientBoostingClassifier, AdaBoostClassifer, RandomForestClassifier, SVC, DecisionTreeClassifier, LogisticRegression y BernoulliNB.\n",
    "3. Existen tres vectores objetivos a evaluar: <code>single</code>, <code>seeing someone</code> y <code>available</code>.\n",
    "4. Serializar el objeto y preservarlo por cada combinación de modelo entrenado y vector objetivo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba09cee-b108-48d5-8b24-a104d247357a",
   "metadata": {},
   "source": [
    "1. Se crearán los conjuntos de atributos y vector objetivo en cada dataframe, considerando que se debe crear un modelo por cada vector objetivo: <code>single</code>, <code>seeing_someone</code> y <code>available</code>.\n",
    "2. Se entrenarán los modelos en cada caso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e970fea0-7d9e-4cf4-afb9-6c9b87288c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 16 ms (started: 2021-12-14 00:06:02 -03:00)\n"
     ]
    }
   ],
   "source": [
    "X_train = train_cupid.drop(columns = [\"single\",\n",
    "                                     \"seeing_someone\",\n",
    "                                     \"available\"])\n",
    "y_train_single = train_cupid[\"single\"]\n",
    "\n",
    "y_train_ssomeone = train_cupid[\"seeing_someone\"]\n",
    "\n",
    "y_train_available = train_cupid[\"available\"]\n",
    "\n",
    "random_seed = 7350"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c6f0aff-0cc1-485b-9437-a5466b28b48f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 7.2 s (started: 2021-12-14 00:06:02 -03:00)\n"
     ]
    }
   ],
   "source": [
    "#modelos Logistic Regression\n",
    "\n",
    "logreg_single = LogisticRegression(penalty = \"l2\",\n",
    "                                   solver = \"newton-cg\",\n",
    "                                   max_iter = 500,\n",
    "                                   random_state = random_seed)\n",
    "logreg_single_fit = logreg_single.fit(X_train, y_train_single)\n",
    "\n",
    "logreg_ssomeone = LogisticRegression(penalty = \"l2\",\n",
    "                                     solver = \"newton-cg\",\n",
    "                                     max_iter = 500,\n",
    "                                     random_state = random_seed)\n",
    "logreg_ssomeone_fit = logreg_ssomeone.fit(X_train, y_train_ssomeone)\n",
    "\n",
    "logreg_available = LogisticRegression(penalty = \"l2\",\n",
    "                                      solver = \"newton-cg\",\n",
    "                                      max_iter = 500,\n",
    "                                      random_state = random_seed)\n",
    "logreg_available_fit = logreg_available.fit(X_train, y_train_available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "54b6a464-9283-407a-95da-7dda51911c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 27 s (started: 2021-12-14 00:06:09 -03:00)\n"
     ]
    }
   ],
   "source": [
    "#modelos Support Vector Classifier\n",
    "\n",
    "svc_single = SVC(random_state = random_seed)\n",
    "svc_single_fit = svc_single.fit(X_train, y_train_single)\n",
    "\n",
    "svc_ssomeone = SVC(random_state = random_seed)\n",
    "svc_ssomeone_fit = svc_ssomeone.fit(X_train, y_train_ssomeone)\n",
    "\n",
    "svc_available = SVC(random_state = random_seed)\n",
    "svc_available_fit = svc_available.fit(X_train, y_train_available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "28215ba2-10f7-4dfc-819f-718b34051811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 515 ms (started: 2021-12-14 00:06:36 -03:00)\n"
     ]
    }
   ],
   "source": [
    "#modelos Bernoulli Naive Bayes\n",
    "\n",
    "bernoullinb_single = BernoulliNB()\n",
    "bernoullinb_single_fit = bernoullinb_single.fit(X_train, y_train_single)\n",
    "\n",
    "bernoullinb_ssomeone = BernoulliNB()\n",
    "bernoullinb_ssomeone_fit = bernoullinb_ssomeone.fit(X_train, y_train_ssomeone)\n",
    "\n",
    "bernoullinb_available = BernoulliNB()\n",
    "bernoullinb_available_fit = bernoullinb_available.fit(X_train, y_train_available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1b940fd6-e110-4b43-8375-9023b28d9af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.34 s (started: 2021-12-14 00:06:37 -03:00)\n"
     ]
    }
   ],
   "source": [
    "#modelos Decision Tree Classifier\n",
    "\n",
    "tree_single = DecisionTreeClassifier(random_state = random_seed)\n",
    "tree_single_fit = tree_single.fit(X_train, y_train_single)\n",
    "\n",
    "tree_ssomeone = DecisionTreeClassifier(random_state = random_seed)\n",
    "tree_ssomeone_fit = tree_ssomeone.fit(X_train, y_train_ssomeone)\n",
    "\n",
    "tree_available = DecisionTreeClassifier(random_state = random_seed)\n",
    "tree_available_fit = tree_available.fit(X_train, y_train_available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e671b51e-fa08-406f-90b8-f822dc1863f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 12.3 s (started: 2021-12-14 00:06:38 -03:00)\n"
     ]
    }
   ],
   "source": [
    "#modelos Random Forest Classifier\n",
    "\n",
    "rf_single = RandomForestClassifier(random_state = random_seed)\n",
    "rf_single_fit = rf_single.fit(X_train, y_train_single)\n",
    "\n",
    "rf_ssomeone = RandomForestClassifier(random_state = random_seed)\n",
    "rf_ssomeone_fit = rf_ssomeone.fit(X_train, y_train_ssomeone)\n",
    "\n",
    "rf_available = RandomForestClassifier(random_state = random_seed)\n",
    "rf_available_fit = rf_available.fit(X_train, y_train_available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "60af85d9-3177-4455-b76d-a41a5ba3353a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 36.9 s (started: 2021-12-14 00:06:51 -03:00)\n"
     ]
    }
   ],
   "source": [
    "#modelos AdaBoost\n",
    "\n",
    "aboost_single = AdaBoostClassifier(random_state = random_seed)\n",
    "aboost_single_fit = aboost_single.fit(X_train, y_train_single)\n",
    "\n",
    "aboost_ssomeone = AdaBoostClassifier(random_state = random_seed)\n",
    "aboost_ssomeone_fit = aboost_ssomeone.fit(X_train, y_train_ssomeone)\n",
    "\n",
    "aboost_available = AdaBoostClassifier(random_state = random_seed)\n",
    "aboost_available_fit = aboost_available.fit(X_train, y_train_available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "97b573a7-bef6-4eaa-a328-6adad4ecdfac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 19.9 s (started: 2021-12-14 00:07:27 -03:00)\n"
     ]
    }
   ],
   "source": [
    "#modelos Gradient Boosting\n",
    "\n",
    "gboost_single = GradientBoostingClassifier(random_state = random_seed)\n",
    "gboost_single_fit = gboost_single.fit(X_train, y_train_single)\n",
    "\n",
    "gboost_ssomeone = GradientBoostingClassifier(random_state = random_seed)\n",
    "gboost_ssomeone_fit = gboost_ssomeone.fit(X_train, y_train_ssomeone)\n",
    "\n",
    "gboost_available = GradientBoostingClassifier(random_state = random_seed)\n",
    "gboost_available_fit = gboost_available.fit(X_train, y_train_available)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa03ca87-28be-4c03-abfd-c161fab3bf1c",
   "metadata": {},
   "source": [
    "Ahora se serializarán los modelos con el módulo <code>joblib</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1d05ed0f-290c-4a3b-b306-68c14f8fd428",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['FP_gboost_available.sav']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 641 ms (started: 2021-12-14 00:07:47 -03:00)\n"
     ]
    }
   ],
   "source": [
    "filename_logreg_single = \"FP_logreg_single.sav\"\n",
    "filename_logreg_ssomeone = \"FP_logreg_ssomeone.sav\"\n",
    "filename_logreg_available = \"FP_logreg_available.sav\"\n",
    "\n",
    "joblib.dump(logreg_single_fit, filename_logreg_single)\n",
    "joblib.dump(logreg_ssomeone_fit, filename_logreg_ssomeone)\n",
    "joblib.dump(logreg_available_fit, filename_logreg_available)\n",
    "\n",
    "filename_svc_single = \"FP_svc_single.sav\"\n",
    "filename_svc_ssomeone = \"FP_svc_ssomeone.sav\"\n",
    "filename_svc_available = \"FP_svc_available.sav\"\n",
    "\n",
    "joblib.dump(svc_single_fit, filename_svc_single)\n",
    "joblib.dump(svc_ssomeone_fit, filename_svc_ssomeone)\n",
    "joblib.dump(svc_available_fit, filename_svc_available)\n",
    "\n",
    "filename_bernoullinb_single = \"FP_bernoullinb_single.sav\"\n",
    "filename_bernoullinb_ssomeone = \"FP_bernoullinb_ssomeone.sav\"\n",
    "filename_bernoullinb_available = \"FP_bernoullinb_available.sav\"\n",
    "\n",
    "joblib.dump(bernoullinb_single_fit, filename_bernoullinb_single)\n",
    "joblib.dump(bernoullinb_ssomeone_fit, filename_bernoullinb_ssomeone)\n",
    "joblib.dump(bernoullinb_available_fit, filename_bernoullinb_available)\n",
    "\n",
    "filename_tree_single = \"FP_tree_single.sav\"\n",
    "filename_tree_ssomeone = \"FP_tree_ssomeone.sav\"\n",
    "filename_tree_available = \"FP_tree_available.sav\"\n",
    "\n",
    "joblib.dump(tree_single_fit, filename_tree_single)\n",
    "joblib.dump(tree_ssomeone_fit, filename_tree_ssomeone)\n",
    "joblib.dump(tree_available_fit, filename_tree_available)\n",
    "\n",
    "filename_rf_single = \"FP_rf_single.sav\"\n",
    "filename_rf_ssomeone = \"FP_rf_ssomeone.sav\"\n",
    "filename_rf_available = \"FP_rf_available.sav\"\n",
    "\n",
    "joblib.dump(rf_single_fit, filename_rf_single)\n",
    "joblib.dump(rf_ssomeone_fit, filename_rf_ssomeone)\n",
    "joblib.dump(rf_available_fit, filename_rf_available)\n",
    "\n",
    "filename_aboost_single = \"FP_aboost_single.sav\"\n",
    "filename_aboost_ssomeone = \"FP_aboost_ssomeone.sav\"\n",
    "filename_aboost_available = \"FP_aboost_available.sav\"\n",
    "\n",
    "joblib.dump(aboost_single_fit, filename_aboost_single)\n",
    "joblib.dump(aboost_ssomeone_fit, filename_aboost_ssomeone)\n",
    "joblib.dump(aboost_available_fit, filename_aboost_available)\n",
    "\n",
    "filename_gboost_single = \"FP_gboost_single.sav\"\n",
    "filename_gboost_ssomeone = \"FP_gboost_ssomeone.sav\"\n",
    "filename_gboost_available = \"FP_gboost_available.sav\"\n",
    "\n",
    "joblib.dump(gboost_single_fit, filename_gboost_single)\n",
    "joblib.dump(gboost_ssomeone_fit, filename_gboost_ssomeone)\n",
    "joblib.dump(gboost_available_fit, filename_gboost_available)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81281aec-aad7-457f-a383-c09513f318f5",
   "metadata": {},
   "source": [
    "**Parte 3: Exportación de predicciones (3.5 Puntos)**\n",
    "\n",
    "1. Ingestar la tabla de testing mediante psycopg2 para la posterior predicción del modelo.\n",
    "2. En base a los objetos serializados, predecir y evaluar cuatro queries específicas:\n",
    "○ Query 1: 'atheism', 'asian', 'employed', 'pro_dogs', 'chinese'.\n",
    "○ Query 2: 'income_over_75', 'french', 'german','orientation_straight', 'new york'.\n",
    "○ Query 3: 'education_undergrad_university', 'body_type_regular', 'pro_dogs',\n",
    "'employed'.\n",
    "○ Query 4: 'taurus', 'indian', 'washington', 'income_between_50_75', 'hinduism'.\n",
    "- Cada una de estas queries específicas debe ser registrada en la base de datos.\n",
    "4. La base de datos creada debe contener las tablas:\n",
    "○ 2 que representan a training y testing.\n",
    "○ 84 que representan a cada una de las combinaciones entre modelo, vector y\n",
    "query específica.\n",
    "> A modo de referencia, la base de datos creada debe contener 86 tablas en total."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd13a11-aa35-43cc-bc0e-eb6efee48096",
   "metadata": {},
   "source": [
    "Con los modelos ya serializados (adjuntos en una de las carpetas de entrega) se crearán las tablas de cada <code>query</code> solicitada, extrayendo la información desde el dataframe <code>test_cupid</code>.\n",
    "\n",
    "Ahora además se definirán los conjuntos de validación, y se crean las predicciones de cada modelo ajustadas a los conjuntos recién creados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0d8062f9-9335-481d-888b-2552204a22d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 15 ms (started: 2021-12-14 00:07:48 -03:00)\n"
     ]
    }
   ],
   "source": [
    "X_test = test_cupid.drop(columns = [\"single\",\n",
    "                                    \"seeing_someone\",\n",
    "                                    \"available\"])\n",
    "\n",
    "y_test_single = test_cupid[\"single\"]\n",
    "\n",
    "y_test_ssomeone = test_cupid[\"seeing_someone\"]\n",
    "\n",
    "y_test_available = test_cupid[\"available\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "28ad1023-6a53-4da5-88ed-e06a19192991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 515 ms (started: 2021-12-14 00:07:48 -03:00)\n"
     ]
    }
   ],
   "source": [
    "#logistic regression, single\n",
    "logreg_single_yhat = logreg_single_fit.predict(X_test)\n",
    "\n",
    "#logistic regression, seeing someone\n",
    "logreg_ssomeone_yhat = logreg_ssomeone_fit.predict(X_test)\n",
    "\n",
    "#logistic regression, available\n",
    "logreg_available_yhat = logreg_available_fit.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8ad6832b-6107-4fbb-8f5c-7e471b7c7fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 43.6 s (started: 2021-12-14 00:07:49 -03:00)\n"
     ]
    }
   ],
   "source": [
    "#support vector machine, single\n",
    "svc_single_yhat = svc_single_fit.predict(X_test)\n",
    "\n",
    "#support vector machine, seeing someone\n",
    "svc_ssomeone_yhat = svc_ssomeone_fit.predict(X_test)\n",
    "\n",
    "#support vector machine, available\n",
    "svc_available_yhat = svc_available_fit.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "51e53898-10f4-4caf-aab9-f364f3d0534d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 515 ms (started: 2021-12-14 00:08:32 -03:00)\n"
     ]
    }
   ],
   "source": [
    "#Bernoulli Naive Bayes, single\n",
    "bernoullinb_single_yhat = bernoullinb_single_fit.predict(X_test)\n",
    "\n",
    "#Bernoulli Naive Bayes, seeing someone\n",
    "bernoullinb_ssomeone_yhat = bernoullinb_ssomeone_fit.predict(X_test)\n",
    "\n",
    "#Bernoulli Naive Bayes, available\n",
    "bernoullinb_available_yhat = bernoullinb_available_fit.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3ec910a8-b4ec-4da0-b445-6c6931cb7753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 500 ms (started: 2021-12-14 00:08:33 -03:00)\n"
     ]
    }
   ],
   "source": [
    "#Decision Tree, single\n",
    "tree_single_yhat = tree_single_fit.predict(X_test)\n",
    "\n",
    "#Decision Tree, seeing someone\n",
    "tree_ssomeone_yhat = tree_ssomeone_fit.predict(X_test)\n",
    "\n",
    "#Decision Tree, available\n",
    "tree_available_yhat = tree_available_fit.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "390b120d-bd31-4a91-963f-85089b265412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.91 s (started: 2021-12-14 00:08:33 -03:00)\n"
     ]
    }
   ],
   "source": [
    "#Random Forest, single\n",
    "rf_single_yhat = rf_single_fit.predict(X_test)\n",
    "\n",
    "#Random Forest, seeing someone\n",
    "rf_ssomeone_yhat = rf_ssomeone_fit.predict(X_test)\n",
    "\n",
    "#Random Forest, available\n",
    "rf_available_yhat = rf_available_fit.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "88d75da1-535c-4604-9c04-5c0832923139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 17.2 s (started: 2021-12-14 00:08:36 -03:00)\n"
     ]
    }
   ],
   "source": [
    "#AdaBoost, single\n",
    "aboost_single_yhat = aboost_single_fit.predict(X_test)\n",
    "\n",
    "#AdaBoost, seeing someone\n",
    "aboost_ssomeone_yhat = aboost_ssomeone_fit.predict(X_test)\n",
    "\n",
    "#AdaBoost, available\n",
    "aboost_available_yhat = aboost_available_fit.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6f4ae843-25dc-4869-a0f9-7930d4aaa74d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 734 ms (started: 2021-12-14 00:08:53 -03:00)\n"
     ]
    }
   ],
   "source": [
    "#Gradient Boosting, single\n",
    "gboost_single_yhat = gboost_single_fit.predict(X_test)\n",
    "\n",
    "#Gradient Boosting, seeing someone\n",
    "gboost_ssomeone_yhat = gboost_ssomeone_fit.predict(X_test)\n",
    "\n",
    "#Gradient Boosting, available\n",
    "gboost_available_yhat = gboost_available_fit.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae726787-d0a3-40d5-afa1-b833701b01f9",
   "metadata": {},
   "source": [
    "Ahora, se crea un dataframe que contenga los nombres de cada predicción y el array correspondiente, recién creado en cada caso con <code>.predict()</code> para reunir la información en un solo elemento.\n",
    "\n",
    "Posteriormente, se crea una lista con los campos que corresponden a cada <code>query</code> para su uso posterior.\n",
    "\n",
    "Junto con ello, cada columna de las predicciones será unida a la tabla <code>test_cupid</code> para poder después hacer las agrupaciones necesarias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5dfb819d-b62d-4cc5-a1b1-67d814bccd9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns (started: 2021-12-14 00:08:54 -03:00)\n"
     ]
    }
   ],
   "source": [
    "predictions = {\"logreg_single_yhat\" : logreg_single_yhat,\n",
    "               \"logreg_ssomeone_yhat\" : logreg_ssomeone_yhat,\n",
    "               \"logreg_available_yhat\" : logreg_available_yhat,\n",
    "               \"svc_single_yhat\" : svc_single_yhat,\n",
    "               \"svc_ssomeone_yhat\" : svc_ssomeone_yhat,\n",
    "               \"svc_available_yhat\" : svc_available_yhat,\n",
    "               \"bernoullinb_single_yhat\" : bernoullinb_single_yhat,\n",
    "               \"bernoullinb_ssomeone_yhat\" : bernoullinb_ssomeone_yhat,\n",
    "               \"bernoullinb_available_yhat\" : bernoullinb_available_yhat,\n",
    "               \"tree_single_yhat\" : tree_single_yhat,\n",
    "               \"tree_ssomeone_yhat\" : tree_ssomeone_yhat,\n",
    "               \"tree_available_yhat\" : tree_available_yhat,\n",
    "               \"rf_single_yhat\" : rf_single_yhat,\n",
    "               \"rf_ssomeone_yhat\" : rf_ssomeone_yhat,\n",
    "               \"rf_available_yhat\" : rf_available_yhat,\n",
    "               \"aboost_single_yhat\" : aboost_single_yhat,\n",
    "               \"aboost_ssomeone_yhat\" : aboost_ssomeone_yhat,\n",
    "               \"aboost_available_yhat\" :aboost_available_yhat,\n",
    "               \"gboost_single_yhat\" : gboost_single_yhat,\n",
    "               \"gboost_ssomeone_yhat\" : gboost_ssomeone_yhat,\n",
    "               \"gboost_available_yhat\" : gboost_available_yhat\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "74b36f24-b09b-46be-beb0-a0d84cf40443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns (started: 2021-12-14 00:08:54 -03:00)\n"
     ]
    }
   ],
   "source": [
    "predictions_df = pd.DataFrame(data = predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6b8c6edc-fcf3-4071-a859-ace74c10ac30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns (started: 2021-12-14 00:08:54 -03:00)\n"
     ]
    }
   ],
   "source": [
    "queries = [[\"atheism\", \"asian\", \"employed\", \"pro_dogs\", \"chinese\"],\n",
    "           [\"income_over_75\", \"french\", \"german\", \"orientation_straight\", \"new_york\"],\n",
    "           [\"education_undergrad_university\", \"body_type_regular\", \"pro_dogs\", \"employed\"],\n",
    "           [\"taurus\", \"indian\", \"washington\", \"income_between_50_75\", \"hinduism\"]\n",
    "          ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ed7b329d-e804-4f12-bbc8-9e689113c469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 312 ms (started: 2021-12-14 00:08:54 -03:00)\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regression\n",
    "\n",
    "df_logreg_single = test_cupid.merge(predictions_df[[\"logreg_single_yhat\"]], left_index = True, right_index = True)\n",
    "df_logreg_ssomeone = test_cupid.merge(predictions_df[[\"logreg_ssomeone_yhat\"]], left_index = True, right_index = True)\n",
    "df_logreg_available = test_cupid.merge(predictions_df[[\"logreg_available_yhat\"]], left_index = True, right_index = True)\n",
    "\n",
    "#Support Vector Classifier\n",
    "\n",
    "df_svc_single = test_cupid.merge(predictions_df[[\"svc_single_yhat\"]], left_index = True, right_index = True)\n",
    "df_svc_ssomeone = test_cupid.merge(predictions_df[[\"svc_ssomeone_yhat\"]], left_index = True, right_index = True)\n",
    "df_svc_available = test_cupid.merge(predictions_df[[\"svc_available_yhat\"]], left_index = True, right_index = True)\n",
    "\n",
    "#Bernoulli Naive Bayes\n",
    "\n",
    "df_bernoullinb_single = test_cupid.merge(predictions_df[[\"bernoullinb_single_yhat\"]], left_index = True, right_index = True)\n",
    "df_bernoullinb_ssomeone = test_cupid.merge(predictions_df[[\"bernoullinb_ssomeone_yhat\"]], left_index = True, right_index = True)\n",
    "df_bernoullinb_available = test_cupid.merge(predictions_df[[\"bernoullinb_available_yhat\"]], left_index = True, right_index = True)\n",
    "\n",
    "#Decision Tree Classifier\n",
    "\n",
    "df_tree_single = test_cupid.merge(predictions_df[[\"tree_single_yhat\"]], left_index = True, right_index = True)\n",
    "df_tree_ssomeone = test_cupid.merge(predictions_df[[\"tree_ssomeone_yhat\"]], left_index = True, right_index = True)\n",
    "df_tree_available = test_cupid.merge(predictions_df[[\"tree_available_yhat\"]], left_index = True, right_index = True)\n",
    "\n",
    "#Random Forest Classifier\n",
    "\n",
    "df_rf_single = test_cupid.merge(predictions_df[[\"rf_single_yhat\"]], left_index = True, right_index = True)\n",
    "df_rf_ssomeone = test_cupid.merge(predictions_df[[\"rf_ssomeone_yhat\"]], left_index = True, right_index = True)\n",
    "df_rf_available = test_cupid.merge(predictions_df[[\"rf_available_yhat\"]], left_index = True, right_index = True)\n",
    "\n",
    "#AdaBoost Classifier\n",
    "\n",
    "df_aboost_single = test_cupid.merge(predictions_df[[\"aboost_single_yhat\"]], left_index = True, right_index = True)\n",
    "df_aboost_ssomeone = test_cupid.merge(predictions_df[[\"aboost_ssomeone_yhat\"]], left_index = True, right_index = True)\n",
    "df_aboost_available = test_cupid.merge(predictions_df[[\"aboost_available_yhat\"]], left_index = True, right_index = True)\n",
    "\n",
    "#Gradient Boosting Classifier\n",
    "\n",
    "df_gboost_single = test_cupid.merge(predictions_df[[\"gboost_single_yhat\"]], left_index = True, right_index = True)\n",
    "df_gboost_ssomeone = test_cupid.merge(predictions_df[[\"gboost_ssomeone_yhat\"]], left_index = True, right_index = True)\n",
    "df_gboost_available = test_cupid.merge(predictions_df[[\"gboost_available_yhat\"]], left_index = True, right_index = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd924cf8-5efd-44df-a306-a58e91082421",
   "metadata": {},
   "source": [
    "Ahora se realizan las agrupaciones, que serán 84 en total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f61464bb-bd7e-40eb-a431-ed6003cba524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 765 ms (started: 2021-12-14 00:08:55 -03:00)\n"
     ]
    }
   ],
   "source": [
    "df_query1_logreg_single = df_logreg_single.groupby(queries[0]).mean()\n",
    "df_query2_logreg_single = df_logreg_single.groupby(queries[1]).mean()\n",
    "df_query3_logreg_single = df_logreg_single.groupby(queries[2]).mean()\n",
    "df_query4_logreg_single = df_logreg_single.groupby(queries[3]).mean()\n",
    "\n",
    "df_query1_logreg_ssomeone = df_logreg_ssomeone.groupby(queries[0]).mean()\n",
    "df_query2_logreg_ssomeone = df_logreg_ssomeone.groupby(queries[1]).mean()\n",
    "df_query3_logreg_ssomeone = df_logreg_ssomeone.groupby(queries[2]).mean()\n",
    "df_query4_logreg_ssomeone = df_logreg_ssomeone.groupby(queries[3]).mean()\n",
    "\n",
    "df_query1_logreg_available = df_logreg_available.groupby(queries[0]).mean()\n",
    "df_query2_logreg_available = df_logreg_available.groupby(queries[1]).mean()\n",
    "df_query3_logreg_available = df_logreg_available.groupby(queries[2]).mean()\n",
    "df_query4_logreg_available = df_logreg_available.groupby(queries[3]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d2d04041-7a04-422c-b572-b34367944c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 735 ms (started: 2021-12-14 00:08:55 -03:00)\n"
     ]
    }
   ],
   "source": [
    "df_query1_svc_single = df_svc_single.groupby(queries[0]).mean()\n",
    "df_query2_svc_single = df_svc_single.groupby(queries[1]).mean()\n",
    "df_query3_svc_single = df_svc_single.groupby(queries[2]).mean()\n",
    "df_query4_svc_single = df_svc_single.groupby(queries[3]).mean()\n",
    "\n",
    "df_query1_svc_ssomeone = df_svc_ssomeone.groupby(queries[0]).mean()\n",
    "df_query2_svc_ssomeone = df_svc_ssomeone.groupby(queries[1]).mean()\n",
    "df_query3_svc_ssomeone = df_svc_ssomeone.groupby(queries[2]).mean()\n",
    "df_query4_svc_ssomeone = df_svc_ssomeone.groupby(queries[3]).mean()\n",
    "\n",
    "df_query1_svc_available = df_svc_available.groupby(queries[0]).mean()\n",
    "df_query2_svc_available = df_svc_available.groupby(queries[1]).mean()\n",
    "df_query3_svc_available = df_svc_available.groupby(queries[2]).mean()\n",
    "df_query4_svc_available = df_svc_available.groupby(queries[3]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fba897ee-db14-43ba-8c13-ca88363aee02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 688 ms (started: 2021-12-14 00:08:56 -03:00)\n"
     ]
    }
   ],
   "source": [
    "df_query1_bernoullinb_single = df_bernoullinb_single.groupby(queries[0]).mean()\n",
    "df_query2_bernoullinb_single = df_bernoullinb_single.groupby(queries[1]).mean()\n",
    "df_query3_bernoullinb_single = df_bernoullinb_single.groupby(queries[2]).mean()\n",
    "df_query4_bernoullinb_single = df_bernoullinb_single.groupby(queries[3]).mean()\n",
    "\n",
    "df_query1_bernoullinb_ssomeone = df_bernoullinb_ssomeone.groupby(queries[0]).mean()\n",
    "df_query2_bernoullinb_ssomeone = df_bernoullinb_ssomeone.groupby(queries[1]).mean()\n",
    "df_query3_bernoullinb_ssomeone = df_bernoullinb_ssomeone.groupby(queries[2]).mean()\n",
    "df_query4_bernoullinb_ssomeone = df_bernoullinb_ssomeone.groupby(queries[3]).mean()\n",
    "\n",
    "df_query1_bernoullinb_available = df_bernoullinb_available.groupby(queries[0]).mean()\n",
    "df_query2_bernoullinb_available = df_bernoullinb_available.groupby(queries[1]).mean()\n",
    "df_query3_bernoullinb_available = df_bernoullinb_available.groupby(queries[2]).mean()\n",
    "df_query4_bernoullinb_available = df_bernoullinb_available.groupby(queries[3]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f1b37871-118f-4d65-bd84-cbcbce43398d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 687 ms (started: 2021-12-14 00:08:57 -03:00)\n"
     ]
    }
   ],
   "source": [
    "df_query1_tree_single = df_tree_single.groupby(queries[0]).mean()\n",
    "df_query2_tree_single = df_tree_single.groupby(queries[1]).mean()\n",
    "df_query3_tree_single = df_tree_single.groupby(queries[2]).mean()\n",
    "df_query4_tree_single = df_tree_single.groupby(queries[3]).mean()\n",
    "\n",
    "df_query1_tree_ssomeone = df_tree_ssomeone.groupby(queries[0]).mean()\n",
    "df_query2_tree_ssomeone = df_tree_ssomeone.groupby(queries[1]).mean()\n",
    "df_query3_tree_ssomeone = df_tree_ssomeone.groupby(queries[2]).mean()\n",
    "df_query4_tree_ssomeone = df_tree_ssomeone.groupby(queries[3]).mean()\n",
    "\n",
    "df_query1_tree_available = df_tree_available.groupby(queries[0]).mean()\n",
    "df_query2_tree_available = df_tree_available.groupby(queries[1]).mean()\n",
    "df_query3_tree_available = df_tree_available.groupby(queries[2]).mean()\n",
    "df_query4_tree_available = df_tree_available.groupby(queries[3]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3d88a4ff-3055-4cec-a930-3c1882da6f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 703 ms (started: 2021-12-14 00:08:57 -03:00)\n"
     ]
    }
   ],
   "source": [
    "df_query1_rf_single = df_rf_single.groupby(queries[0]).mean()\n",
    "df_query2_rf_single = df_rf_single.groupby(queries[1]).mean()\n",
    "df_query3_rf_single = df_rf_single.groupby(queries[2]).mean()\n",
    "df_query4_rf_single = df_rf_single.groupby(queries[3]).mean()\n",
    "\n",
    "df_query1_rf_ssomeone = df_rf_ssomeone.groupby(queries[0]).mean()\n",
    "df_query2_rf_ssomeone = df_rf_ssomeone.groupby(queries[1]).mean()\n",
    "df_query3_rf_ssomeone = df_rf_ssomeone.groupby(queries[2]).mean()\n",
    "df_query4_rf_ssomeone = df_rf_ssomeone.groupby(queries[3]).mean()\n",
    "\n",
    "df_query1_rf_available = df_rf_available.groupby(queries[0]).mean()\n",
    "df_query2_rf_available = df_rf_available.groupby(queries[1]).mean()\n",
    "df_query3_rf_available = df_rf_available.groupby(queries[2]).mean()\n",
    "df_query4_rf_available = df_rf_available.groupby(queries[3]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c16c1da8-85fc-4e93-9245-d959b01982c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 687 ms (started: 2021-12-14 00:08:58 -03:00)\n"
     ]
    }
   ],
   "source": [
    "df_query1_aboost_single = df_aboost_single.groupby(queries[0]).mean()\n",
    "df_query2_aboost_single = df_aboost_single.groupby(queries[1]).mean()\n",
    "df_query3_aboost_single = df_aboost_single.groupby(queries[2]).mean()\n",
    "df_query4_aboost_single = df_aboost_single.groupby(queries[3]).mean()\n",
    "\n",
    "df_query1_aboost_ssomeone = df_aboost_ssomeone.groupby(queries[0]).mean()\n",
    "df_query2_aboost_ssomeone = df_aboost_ssomeone.groupby(queries[1]).mean()\n",
    "df_query3_aboost_ssomeone = df_aboost_ssomeone.groupby(queries[2]).mean()\n",
    "df_query4_aboost_ssomeone = df_aboost_ssomeone.groupby(queries[3]).mean()\n",
    "\n",
    "df_query1_aboost_available = df_aboost_available.groupby(queries[0]).mean()\n",
    "df_query2_aboost_available = df_aboost_available.groupby(queries[1]).mean()\n",
    "df_query3_aboost_available = df_aboost_available.groupby(queries[2]).mean()\n",
    "df_query4_aboost_available = df_aboost_available.groupby(queries[3]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e156c6de-e368-462a-98b5-66e76812c685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 719 ms (started: 2021-12-14 00:08:59 -03:00)\n"
     ]
    }
   ],
   "source": [
    "df_query1_gboost_single = df_gboost_single.groupby(queries[0]).mean()\n",
    "df_query2_gboost_single = df_gboost_single.groupby(queries[1]).mean()\n",
    "df_query3_gboost_single = df_gboost_single.groupby(queries[2]).mean()\n",
    "df_query4_gboost_single = df_gboost_single.groupby(queries[3]).mean()\n",
    "\n",
    "df_query1_gboost_ssomeone = df_gboost_ssomeone.groupby(queries[0]).mean()\n",
    "df_query2_gboost_ssomeone = df_gboost_ssomeone.groupby(queries[1]).mean()\n",
    "df_query3_gboost_ssomeone = df_gboost_ssomeone.groupby(queries[2]).mean()\n",
    "df_query4_gboost_ssomeone = df_gboost_ssomeone.groupby(queries[3]).mean()\n",
    "\n",
    "df_query1_gboost_available = df_gboost_available.groupby(queries[0]).mean()\n",
    "df_query2_gboost_available = df_gboost_available.groupby(queries[1]).mean()\n",
    "df_query3_gboost_available = df_gboost_available.groupby(queries[2]).mean()\n",
    "df_query4_gboost_available = df_gboost_available.groupby(queries[3]).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d6930a-95ce-4dde-a525-b5475b6f3e20",
   "metadata": {},
   "source": [
    "Finalmente se crea el <code>engine</code> para exportar las agrupaciones a <code>postgreSQL</code> junto con listas que permitan exportar la información, que contengan cada agrupación y su nombre correspondiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e11e1ecd-4e84-446b-92c8-a3e63fa89640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns (started: 2021-12-14 00:16:11 -03:00)\n"
     ]
    }
   ],
   "source": [
    "engine = create_engine(\"postgresql://postgres:1163143094161049@localhost:5432/pinto_francisca\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "80e7643f-b198-4c16-99d1-56dc1f604f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns (started: 2021-12-14 00:09:00 -03:00)\n"
     ]
    }
   ],
   "source": [
    "dataframes = [df_query1_logreg_single,\n",
    "              df_query2_logreg_single,\n",
    "              df_query3_logreg_single,\n",
    "              df_query4_logreg_single,\n",
    "              df_query1_logreg_ssomeone,\n",
    "              df_query2_logreg_ssomeone,\n",
    "              df_query3_logreg_ssomeone,\n",
    "              df_query4_logreg_ssomeone,\n",
    "              df_query1_logreg_available,\n",
    "              df_query2_logreg_available,\n",
    "              df_query3_logreg_available,\n",
    "              df_query4_logreg_available,\n",
    "              df_query1_svc_single,\n",
    "              df_query2_svc_single,\n",
    "              df_query3_svc_single,\n",
    "              df_query4_svc_single,\n",
    "              df_query1_svc_ssomeone,\n",
    "              df_query2_svc_ssomeone,\n",
    "              df_query3_svc_ssomeone,\n",
    "              df_query4_svc_ssomeone,\n",
    "              df_query1_svc_available,\n",
    "              df_query2_svc_available,\n",
    "              df_query3_svc_available,\n",
    "              df_query4_svc_available,\n",
    "              df_query1_bernoullinb_single,\n",
    "              df_query2_bernoullinb_single,\n",
    "              df_query3_bernoullinb_single,\n",
    "              df_query4_bernoullinb_single,\n",
    "              df_query1_bernoullinb_ssomeone,\n",
    "              df_query2_bernoullinb_ssomeone,\n",
    "              df_query3_bernoullinb_ssomeone,\n",
    "              df_query4_bernoullinb_ssomeone,\n",
    "              df_query1_bernoullinb_available,\n",
    "              df_query2_bernoullinb_available,\n",
    "              df_query3_bernoullinb_available,\n",
    "              df_query4_bernoullinb_available,\n",
    "              df_query1_tree_single,\n",
    "              df_query2_tree_single,\n",
    "              df_query3_tree_single,\n",
    "              df_query4_tree_single,\n",
    "              df_query1_tree_ssomeone,\n",
    "              df_query2_tree_ssomeone,\n",
    "              df_query3_tree_ssomeone,\n",
    "              df_query4_tree_ssomeone,\n",
    "              df_query1_tree_available,\n",
    "              df_query2_tree_available,\n",
    "              df_query3_tree_available,\n",
    "              df_query4_tree_available,\n",
    "              df_query1_rf_single,\n",
    "              df_query2_rf_single,\n",
    "              df_query3_rf_single,\n",
    "              df_query4_rf_single,\n",
    "              df_query1_rf_ssomeone,\n",
    "              df_query2_rf_ssomeone,\n",
    "              df_query3_rf_ssomeone,\n",
    "              df_query4_rf_ssomeone,\n",
    "              df_query1_rf_available,\n",
    "              df_query2_rf_available,\n",
    "              df_query3_rf_available,\n",
    "              df_query4_rf_available,\n",
    "              df_query1_aboost_single,\n",
    "              df_query2_aboost_single,\n",
    "              df_query3_aboost_single,\n",
    "              df_query4_aboost_single,\n",
    "              df_query1_aboost_ssomeone,\n",
    "              df_query2_aboost_ssomeone,\n",
    "              df_query3_aboost_ssomeone,\n",
    "              df_query4_aboost_ssomeone,\n",
    "              df_query1_aboost_available,\n",
    "              df_query2_aboost_available,\n",
    "              df_query3_aboost_available,\n",
    "              df_query4_aboost_available,\n",
    "              df_query1_gboost_single,\n",
    "              df_query2_gboost_single,\n",
    "              df_query3_gboost_single,\n",
    "              df_query4_gboost_single,\n",
    "              df_query1_gboost_ssomeone,\n",
    "              df_query2_gboost_ssomeone,\n",
    "              df_query3_gboost_ssomeone,\n",
    "              df_query4_gboost_ssomeone,\n",
    "              df_query1_gboost_available,\n",
    "              df_query2_gboost_available,\n",
    "              df_query3_gboost_available,\n",
    "              df_query4_gboost_available,\n",
    "              ]\n",
    "\n",
    "sql_db_names = [\"query1_logreg_single\",\n",
    "                \"query2_logreg_single\",\n",
    "                \"query3_logreg_single\",\n",
    "                \"query4_logreg_single\",\n",
    "                \"query1_logreg_ssomeone\",\n",
    "                \"query2_logreg_ssomeone\",\n",
    "                \"query3_logreg_ssomeone\",\n",
    "                \"query4_logreg_ssomeone\",\n",
    "                \"query1_logreg_available\",\n",
    "                \"query2_logreg_available\",\n",
    "                \"query3_logreg_available\",\n",
    "                \"query4_logreg_available\",\n",
    "                \"query1_svc_single\",\n",
    "                \"query2_svc_single\",\n",
    "                \"query3_svc_single\",\n",
    "                \"query4_svc_single\",\n",
    "                \"query1_svc_ssomeone\",\n",
    "                \"query2_svc_ssomeone\",\n",
    "                \"query3_svc_ssomeone\",\n",
    "                \"query4_svc_ssomeone\",\n",
    "                \"query1_svc_available\",\n",
    "                \"query2_svc_available\",\n",
    "                \"query3_svc_available\",\n",
    "                \"query4_svc_available\",\n",
    "                \"query1_bernoullinb_single\",\n",
    "                \"query2_bernoullinb_single\",\n",
    "                \"query3_bernoullinb_single\",\n",
    "                \"query4_bernoullinb_single\",\n",
    "                \"query1_bernoullinb_ssomeone\",\n",
    "                \"query2_bernoullinb_ssomeone\",\n",
    "                \"query3_bernoullinb_ssomeone\",\n",
    "                \"query4_bernoullinb_ssomeone\",\n",
    "                \"query1_bernoullinb_available\",\n",
    "                \"query2_bernoullinb_available\",\n",
    "                \"query3_bernoullinb_available\",\n",
    "                \"query4_bernoullinb_available\",\n",
    "                \"query1_tree_single\",\n",
    "                \"query2_tree_single\",\n",
    "                \"query3_tree_single\",\n",
    "                \"query4_tree_single\",\n",
    "                \"query1_tree_ssomeone\",\n",
    "                \"query2_tree_ssomeone\",\n",
    "                \"query3_tree_ssomeone\",\n",
    "                \"query4_tree_ssomeone\",\n",
    "                \"query1_tree_available\",\n",
    "                \"query2_tree_available\",\n",
    "                \"query3_tree_available\",\n",
    "                \"query4_tree_available\",\n",
    "                \"query1_rf_single\",\n",
    "                \"query2_rf_single\",\n",
    "                \"query3_rf_single\",\n",
    "                \"query4_rf_single\",\n",
    "                \"query1_rf_ssomeone\",\n",
    "                \"query2_rf_ssomeone\",\n",
    "                \"query3_rf_ssomeone\",\n",
    "                \"query4_rf_ssomeone\",\n",
    "                \"query1_rf_available\",\n",
    "                \"query2_rf_available\",\n",
    "                \"query3_rf_available\",\n",
    "                \"query4_rf_available\",\n",
    "                \"query1_aboost_single\",\n",
    "                \"query2_aboost_single\",\n",
    "                \"query3_aboost_single\",\n",
    "                \"query4_aboost_single\",\n",
    "                \"query1_aboost_ssomeone\",\n",
    "                \"query2_aboost_ssomeone\",\n",
    "                \"query3_aboost_ssomeone\",\n",
    "                \"query4_aboost_ssomeone\",\n",
    "                \"query1_aboost_available\",\n",
    "                \"query2_aboost_available\",\n",
    "                \"query3_aboost_available\",\n",
    "                \"query4_aboost_available\",\n",
    "                \"query1_gboost_single\",\n",
    "                \"query2_gboost_single\",\n",
    "                \"query3_gboost_single\",\n",
    "                \"query4_gboost_single\",\n",
    "                \"query1_gboost_ssomeone\",\n",
    "                \"query2_gboost_ssomeone\",\n",
    "                \"query3_gboost_ssomeone\",\n",
    "                \"query4_gboost_ssomeone\",\n",
    "                \"query1_gboost_available\",\n",
    "                \"query2_gboost_available\",\n",
    "                \"query3_gboost_available\",\n",
    "                \"query4_gboost_available\",\n",
    "                ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8a3baa-d017-49fa-877d-45d0cc3d9679",
   "metadata": {},
   "source": [
    "Finalmente, para crear las distintas tablas en <code>postgreSQL</code> se utiliza una función que repite la operación, iterando con cada DataFrame agrupado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "76b80d79-2d2e-43c5-8fe1-b5550b1b46e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns (started: 2021-12-14 00:16:21 -03:00)\n"
     ]
    }
   ],
   "source": [
    "def create_tables(dataframes, sql_db_names):\n",
    "    for i in dataframes:\n",
    "        for j in sql_db_names:\n",
    "            df_table = i.to_sql(\"{}\".format(j), con = engine, if_exists = \"replace\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9250b15b-db03-443e-a9f0-bde6cbca6fce",
   "metadata": {},
   "source": [
    "La función luego de ser creada se ejecuta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bd4e8769-cf76-4fe1-84f4-1ab2aaea386f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 17min 2s (started: 2021-12-14 00:16:25 -03:00)\n"
     ]
    }
   ],
   "source": [
    "create_tables(dataframes, sql_db_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1beb56d6-48c2-4a3c-9c14-0202d799ac40",
   "metadata": {},
   "source": [
    "Para revisar resultados finales, consultar archivo de texto."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
